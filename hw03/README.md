### Инструкции
1. Поставьте питоновские модули poster, mwclient и mwparserfromhell
2. запустите серверы DFS.
3. создайте подкаталог tmp/plist 
4. определите константу USERNAME в скриптах `mr_posting_lists.py` и `print-posting-list.py`
5. скачайте корпус командой `python crawl-corpus.py` (результаты будут записаны в DFS)
6. сделайте списки вхождений, запустив map-reduce `mr_posting_lists.py`. Обратите внимание, что
он состоит из двух шагов и соответственно запускать рабочий процесс (`python mincemeat.py localhost`) надо дважды. Результаты первого шага
записываются на диск в каталог `tmp/plist`, результаты второго шага записываются в DFS
7. посмотрите на какой-нибудь список, например для терма HANA командой `python print-posting-list.py --term HANA`

### Подводные камни
1. если у вас Windows, то некоторые файлы, создаваемые первым MR в `mr_posting_lists.py` в каталоге `tmp/plists` 
могут превысить ограничение на длину полного пути до файла (зависит от того, где расположен репозиторий на диске)
В таком случае поиграйтесь с константой в функции `reducefn`: `if len(k) > 100:`
2. файлы в DFS только наращиваются (не перетираются и не удаляются), поэтому если хотите начать процесс построения индекса 
"с чистого листа" то не забудьте стереть физические файлы из каталогов, используемых DFS и перезапустить DFS. `rm files data/* data2/*`, запущенный 
из `../dfs` удалит всё, например. Это также означает, что процесс скачивания корпуса или построения списков вхождений не идемпотентный.

